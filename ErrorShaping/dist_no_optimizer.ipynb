{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dist_no_optimizer.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPLbvFmS5n78",
        "colab_type": "code",
        "outputId": "7128432c-9c41-4abd-f168-f3d54652e098",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "%matplotlib inline\n",
        "%load_ext tensorboard\n",
        "!pip install -q tensorflow==2.0.0-beta1\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import sys, os, datetime, time, scipy\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from scipy import stats\n",
        "from scipy.integrate import quad"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 87.9MB 1.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 501kB 44.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.1MB 42.7MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjvkZC03Q54i",
        "colab_type": "text"
      },
      "source": [
        "### Benchmark Errors (MSE only)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtbI4Gaa5lO9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_history(history):\n",
        "  hist = pd.DataFrame(history.history)\n",
        "  hist['epoch'] = history.epoch\n",
        "  plt.figure()\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.plot(hist['epoch'], hist['loss'], label='Train Error')\n",
        "  plt.plot(hist['epoch'], hist['val_loss'], label = 'Val Error')\n",
        "  plt.ylim([0,np.max(hist['loss'])])\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "def build_model(if_print=False):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(5, activation='relu', input_shape=[1]),\n",
        "    tf.keras.layers.Dense(1)])\n",
        "  #optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
        "  optimizer = tf.keras.optimizers.SGD(0.001)\n",
        "  print(optimizer)\n",
        "  model.compile(loss='mse',optimizer=optimizer)\n",
        "  if if_print: model.summary()\n",
        "  return model\n",
        "\n",
        "def get_data(data_size=10000, seed=0):\n",
        "  f=lambda x:np.sin(x)/(1+x**2)\n",
        "  np.random.seed(seed)\n",
        "  x = np.random.uniform(-1, 1, data_size)\n",
        "  y = [f(i) for i in x]\n",
        "  print(\"x[:2]:\", x[:2])\n",
        "  return x,y\n",
        "\n",
        "def get_benchmark_NN(x, y):\n",
        "  model = build_model(1)\n",
        "  history = model.fit(x, y, batch_size = 500, epochs=20000, validation_split = 0.2, verbose=0)\n",
        "  plot_history(history)\n",
        "   \n",
        "  return model\n",
        "\n",
        "def print_model(model):\n",
        "  model.summary()\n",
        "  print(\"layer 1 weights:\", ', '.join([str(item) for item in model.layers[0].trainable_variables[0].numpy()]))\n",
        "  print(\"layer 1 bias:\", ', '.join([str(item) for item in model.layers[0].trainable_variables[1].numpy()]))\n",
        "  print(\"layer 2 weights:\", ', '.join([str(item) for item in model.layers[1].trainable_variables[0].numpy()]))\n",
        "  print(\"layer 2 bias:\", ', '.join([str(item) for item in model.layers[1].trainable_variables[1].numpy()]))\n",
        "    \n",
        "def print_first_layer(model):\n",
        "  print(\"First layer:\", ', '.join([str(item) for item in model.layers[0].trainable_variables[0].numpy()]))\n",
        "    \n",
        "def get_errors(model, x, y, if_print=False):\n",
        "  y_pred = model.predict(x)\n",
        "  y_pred = y_pred.reshape(-1)\n",
        "  errors = y - y_pred\n",
        "  mean = np.mean(errors)\n",
        "  sigma = np.sqrt(np.var(errors))\n",
        "  if if_print:\n",
        "    print(\"\\nmean of errors:\", mean)\n",
        "    print(\"standard deviation of errors:\", sigma)\n",
        "  \n",
        "  return errors, mean, sigma\n",
        "\n",
        "def get_kde(errors):\n",
        "  kde=stats.gaussian_kde(errors)\n",
        "  kde.set_bandwidth(bw_method=kde.factor/10)\n",
        "  return kde\n",
        "def plot_errors(errors):\n",
        "  errors_sigma = np.sqrt(np.var(errors))\n",
        "  plt.figure(figsize=(20, 8))\n",
        "  plt.xticks(fontsize=10)\n",
        "  plt.subplots_adjust(hspace=1)\n",
        "\n",
        "  p1 = plt.subplot(311)\n",
        "  p1.title.set_text(\"Errors Histogram\")\n",
        "  weights = np.ones_like(errors)/float(len(errors))\n",
        "  _, _, _ = plt.hist(errors, weights=weights,bins=100, color='red')\n",
        "\n",
        "  p2 = plt.subplot(312)\n",
        "  p2.title.set_text('Errors PDF')\n",
        "\n",
        "  x1 = np.linspace(np.min(errors), np.max(errors), num=10000)\n",
        "  kde = get_kde(errors)\n",
        "  y1 = kde(x1)\n",
        "  plt.plot(x1, y1, color='black', linewidth=1)\n",
        "  plt.plot(errors, np.full_like(errors, -0.1), '|k', markeredgewidth=1)\n",
        "\n",
        "  p3 = plt.subplot(313)\n",
        "  p3.title.set_text(\"Normal Distribution\")\n",
        "  y_norm = stats.norm.pdf(x1, 0, errors_sigma/4)\n",
        "  plt.plot(x1, y1, x1, y_norm, color='black', linewidth=1)\n",
        "  plt.fill_between(x1, y1, y_norm, where=y_norm >= y1, facecolor='blue', interpolate=True)\n",
        "  plt.fill_between(x1, y1, y_norm, where=y_norm <= y1, facecolor='red', interpolate=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKU0wY2bnfDW",
        "colab_type": "text"
      },
      "source": [
        "### Benchmark Loss (Target Normal) and Gradient"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjfIEdh4F2YM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_loss(errors, if_plot=False):\n",
        "  def normal_minus_kde(x, mean, std, errors):\n",
        "    kde = get_kde(errors)\n",
        "    value = np.square(scipy.stats.norm.pdf(x,mean,std) - kde(x))\n",
        "\n",
        "    return value\n",
        "  \n",
        "  errors_sigma = np.sqrt(np.var(errors))\n",
        "  loss, err = quad(normal_minus_kde, -1, 1, args=(0, errors_sigma/4, errors))\n",
        "\n",
        "  if if_plot:\n",
        "    kde = get_kde(errors)\n",
        "    x1 = np.linspace(np.min(errors), np.max(errors), num=10000)\n",
        "    y1 = kde(x1)\n",
        "    \n",
        "    plt.figure(figsize=(20, 4))\n",
        "    plt.xticks(fontsize=10)\n",
        "    plt.subplots_adjust(hspace=0.3)\n",
        "\n",
        "    p1 = plt.subplot(211)\n",
        "    diffs=[np.abs(stats.norm.pdf(i, 0, errors_sigma/4)-kde(i))[0] for i in x1]\n",
        "    p1.plot(x1, diffs)\n",
        "    plt.xlim(np.min(errors), np.max(errors))\n",
        "    p1.title.set_text(\"Difference between Normal and Errors\")\n",
        "\n",
        "    p2 = plt.subplot(212)\n",
        "    y_norm = stats.norm.pdf(x1, 0, errors_sigma/4)\n",
        "    p2.plot(x1, y1, x1, y_norm, color='black', linewidth=1)\n",
        "    p2.fill_between(x1, y1, y_norm, where=y_norm >= y1, facecolor='blue', interpolate=True)\n",
        "    p2.fill_between(x1, y1, y_norm, where=y_norm <= y1, facecolor='red', interpolate=True)\n",
        "    plt.xlim(np.min(errors), np.max(errors))\n",
        "  \n",
        "  return loss\n",
        "\n",
        "def node_plus_delta(model, layer_n, wb_n, node_n, if_print=False, delta = 10**(-6)):\n",
        "\n",
        "  weight_old = model.layers[layer_n].trainable_variables[wb_n].numpy()\n",
        "  delta_extend = np.zeros(weight_old.shape)\n",
        "  \n",
        "  if layer_n == 0:\n",
        "    if wb_n==0:\n",
        "      delta_extend[0][node_n] = delta\n",
        "    elif wb_n==1:\n",
        "      delta_extend[node_n] = delta\n",
        "  elif layer_n == 1:\n",
        "    if wb_n==0:  \n",
        "      delta_extend[node_n] = delta\n",
        "    elif wb_n==1:\n",
        "      delta_extend = delta\n",
        "      \n",
        "  weight_new = weight_old + delta_extend\n",
        "  if if_print: print(\"\\n OLD: model.layers[\", layer_n, \"].trainable_variables[\", wb_n, \"] with node_n as\", node_n, \":\\n\", model.layers[layer_n].trainable_variables[wb_n])\n",
        "  model.layers[layer_n].trainable_variables[wb_n].assign(weight_new)\n",
        "  if if_print: print(\"\\n NEW: model.layers[\", layer_n, \"].trainable_variables[\", wb_n, \"] with node_n as\", node_n, \":\\n\", model.layers[layer_n].trainable_variables[wb_n])\n",
        "    \n",
        "  return model\n",
        "\n",
        "def node_minus_delta(model, layer_n, wb_n, node_n, if_print=False, delta = 10**(-6)):\n",
        "  weight_old = model.layers[layer_n].trainable_variables[wb_n].numpy()\n",
        "  delta_extend = np.zeros(weight_old.shape)\n",
        "\n",
        "  if layer_n == 0:\n",
        "    if wb_n==0:\n",
        "      delta_extend[0][node_n] = delta\n",
        "    elif wb_n==1:\n",
        "      delta_extend[node_n] = delta\n",
        "  elif layer_n == 1:\n",
        "    if wb_n==0:  \n",
        "      delta_extend[node_n] = delta\n",
        "    elif wb_n==1:\n",
        "      delta_extend = delta\n",
        "    \n",
        "  weight_new = weight_old - delta_extend\n",
        "  if if_print: print(\"\\nmodel.layers[\", layer_n, \"].trainable_variables[\", wb_n, \"] with node_n as\", node_n, \":\\n\", model.layers[layer_n].trainable_variables[wb_n])\n",
        "  model.layers[layer_n].trainable_variables[wb_n].assign(weight_new)\n",
        "  if if_print: print(\"\\nmodel.layers[\", layer_n, \"].trainable_variables[\", wb_n, \"] with node_n as\", node_n, \":\\n\", model.layers[layer_n].trainable_variables[wb_n])\n",
        "  return model\n",
        "\n",
        "def node_plus_delta_loss(model, x, y, layer_n, wb_n, node_n, delta_n=1):\n",
        "  for i in range(delta_n):\n",
        "    model = node_plus_delta(model, layer_n, wb_n, node_n, if_print=0)\n",
        "  errors, mean, var = get_errors(model, x, y)\n",
        "  loss = get_loss(errors)\n",
        "\n",
        "  return loss\n",
        "\n",
        "def check_node_loss(model, x, y, layer_n, wb_n, node_n, benchmark_loss):\n",
        "  node_loss = node_plus_delta_loss(model, x, y, layer_n, wb_n, node_n)\n",
        "  delta_n = 1\n",
        "  n=0\n",
        "  \n",
        "  while benchmark_loss == node_loss and n<10: \n",
        "    n+=1\n",
        "    delta_n+=1\n",
        "    node_loss = node_plus_delta_loss(model, x, y, layer_n, wb_n, node_n, delta_n)\n",
        "  \n",
        "    if n==10:\n",
        "      while benchmark_loss == node_loss and n<20: \n",
        "        n+=1\n",
        "        delta_n+=10\n",
        "        node_loss = node_plus_delta_loss(model, x, y, layer_n, wb_n, node_n, delta_n)\n",
        "\n",
        "    if n==20:\n",
        "      while benchmark_loss == node_loss and n<30: \n",
        "        n+=1\n",
        "        delta_n+=100\n",
        "        node_loss = node_plus_delta_loss(model, x, y, layer_n, wb_n, node_n, delta_n)\n",
        "\n",
        "    if n==30:\n",
        "      while benchmark_loss == node_loss and n<40: \n",
        "        n+=1\n",
        "        delta_n+=1000\n",
        "        node_loss = node_plus_delta_loss(model, x, y, layer_n, wb_n, node_n, delta_n)\n",
        "  \n",
        "  return node_loss, delta_n\n",
        "\n",
        "def nodes_plus_delta_loss(model, x, y, benchmark_loss):\n",
        "  layer_0_w_losses = []\n",
        "  layer_0_b_losses = []\n",
        "  layer_1_w_losses = []\n",
        "  layer_1_b_losses = []\n",
        "  \n",
        "  layer_0_w_delta_n = []\n",
        "  layer_0_b_delta_n = []\n",
        "  layer_1_w_delta_n = []\n",
        "  layer_1_b_delta_n = []\n",
        "  \n",
        "  for i in range(5):\n",
        "    loss, delta_n = check_node_loss(model, x, y, 0, 0, i, benchmark_loss=benchmark_loss)\n",
        "    layer_0_w_losses.append(loss)\n",
        "    layer_0_w_delta_n.append(delta_n)\n",
        "\n",
        "  for i in range(5):\n",
        "    loss, delta_n = check_node_loss(model, x, y, 0, 1, i, benchmark_loss=benchmark_loss)\n",
        "    layer_0_b_losses.append(loss)\n",
        "    layer_0_b_delta_n.append(delta_n)\n",
        "\n",
        "  for i in range(5):\n",
        "    loss, delta_n = check_node_loss(model, x, y, 1, 0, i, benchmark_loss=benchmark_loss)\n",
        "    layer_1_w_losses.append(loss)\n",
        "    layer_1_w_delta_n.append(delta_n)\n",
        "   \n",
        "  loss, delta_n = check_node_loss(model, x, y, 1, 1, 0, benchmark_loss=benchmark_loss)                                \n",
        "  layer_1_b_losses.append(loss)\n",
        "  layer_1_b_delta_n.append(delta_n)\n",
        "  \n",
        "  nodes_losses = [layer_0_w_losses, layer_0_b_losses, layer_1_w_losses, layer_1_b_losses]\n",
        "  nodes_delta_n = [layer_0_w_delta_n, layer_0_b_delta_n, layer_1_w_delta_n, layer_1_b_delta_n]\n",
        "  return nodes_losses,nodes_delta_n\n",
        "\n",
        "def get_gradient(nodes_losses, nodes_delta_n, benchmark_loss):\n",
        "  gradients = []\n",
        "  for i in range(len(nodes_losses)):\n",
        "    grads = []\n",
        "    for j in range(len(nodes_losses[i])):\n",
        "      grad = (nodes_losses[i][j]-benchmark_loss)/(nodes_delta_n[i][j]**10**(-6))\n",
        "      grads.append(grad)\n",
        "    gradients.append(grads)\n",
        "    \n",
        "  return gradients\n",
        "\n",
        "def update_nodes(model, grads, nodes_delta_n, if_print = False, lr = 0.001):\n",
        "  if if_print: \n",
        "    print(\"\\n OLD:\")\n",
        "    print(\"model.layers[0].trainable_variables[0]:\", model.layers[0].trainable_variables[0])\n",
        "    print(\"model.layers[0].trainable_variables[1]:\", model.layers[0].trainable_variables[1])\n",
        "    print(\"model.layers[1].trainable_variables[0]:\", model.layers[1].trainable_variables[0])\n",
        "    print(\"model.layers[1].trainable_variables[1]:\", model.layers[1].trainable_variables[1])\n",
        "  for i in range(len(grads)):\n",
        "    for j in range(len(grads[i])):\n",
        "      delta = lr*grads[i][j]*nodes_delta_n[i][j]*10**(-6)\n",
        "      #print(\"grad:\", grads[i][j])\n",
        "      #print(\"delta:\", delta)\n",
        "      model = node_plus_delta(model, layer_n=0 if i<2 else 1, wb_n=i%2, node_n=j, delta = -delta)\n",
        "      \n",
        "  if if_print: \n",
        "    print(\"\\n NEW:\")\n",
        "    print(\"model.layers[0].trainable_variables[0]:\", model.layers[0].trainable_variables[0])\n",
        "    print(\"model.layers[0].trainable_variables[1]:\", model.layers[0].trainable_variables[1])\n",
        "    print(\"model.layers[1].trainable_variables[0]:\", model.layers[1].trainable_variables[0])\n",
        "    print(\"model.layers[1].trainable_variables[1]:\", model.layers[1].trainable_variables[1])\n",
        "    \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-yE8eY6957i",
        "colab_type": "code",
        "outputId": "6ca2c45d-92ba-4cff-8dcd-3e14f90b4d9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "def train_dist(step_n=50):\n",
        "  \n",
        "  x, y = get_data(1000)\n",
        "  model = get_benchmark_NN(x, y)\n",
        "  print_model(model)\n",
        "  errors, mean, sigma = get_errors(model, x, y, if_print=1)\n",
        "  errors_first = errors\n",
        "\n",
        "  benchmark_loss = get_loss(errors, if_plot=1)\n",
        "  print(\"\\nInitial loss:\", benchmark_loss)\n",
        "  \n",
        "  lr_init = 0.1\n",
        "  lr = lr_init\n",
        "  patience_init = 20\n",
        "  patience = patience_init\n",
        "  losses=[]; errors_mses=[]; errors_means=[]; errors_all = []\n",
        "  losses.append(benchmark_loss); errors_mses.append(sigma**2); errors_means.append(mean); errors_all.append(errors)\n",
        "  for i in range(step_n):\n",
        "    print(\"-----Step\", i)\n",
        "    if i < (step_n-1): if_print=0\n",
        "    else: if_print=0\n",
        "      \n",
        "    # estimate gradient\n",
        "    nodes_losses, nodes_delta_n = nodes_plus_delta_loss(model, x, y, benchmark_loss)\n",
        "    grads = get_gradient(nodes_losses, nodes_delta_n, benchmark_loss)\n",
        "    model = update_nodes(model, grads, nodes_delta_n, if_print=if_print, lr=lr)\n",
        "    \n",
        "    # get errors and losses\n",
        "    errors, mean, sigma = get_errors(model, x, y, if_print=if_print)\n",
        "    new_loss = get_loss(errors, if_plot=if_print)\n",
        "    if new_loss < np.min(losses): patience = patience_init\n",
        "    else: patience-= 1\n",
        "     \n",
        "    # record results\n",
        "    losses.append(new_loss)\n",
        "    errors_mses.append(sigma**2)\n",
        "    errors_means.append(mean)\n",
        "    errors_all.append(errors)\n",
        "    print(\"New loss:\", new_loss, \"    (learning rate:\", lr, \")\")\n",
        "    if patience <0: break\n",
        "  \n",
        "  print_model(model)\n",
        "  errors_last = errors\n",
        "  xs = np.arange(len(losses)+1)\n",
        "\n",
        "  plt.figure(dpi=100, figsize=(20, 2))\n",
        "  plt.xticks(fontsize=10)\n",
        "  plt.subplots_adjust(wspace=0.2)\n",
        "\n",
        "  p1 = plt.subplot(131)\n",
        "  p1.plot(xs, losses)\n",
        "  plt.xlabel(\"step\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  \n",
        "  p2 = plt.subplot(132)\n",
        "  p2.plot(xs, errors_mses)\n",
        "  plt.xlabel(\"step\")\n",
        "  plt.ylabel(\"Errors MSE\")\n",
        "\n",
        "  p3 = plt.subplot(133)\n",
        "  p3.plot(xs, errors_means)\n",
        "  plt.xlabel(\"step\")\n",
        "  plt.ylabel(\"Errors Mean\")\n",
        "\n",
        "  return losses, errors_all, model\n",
        "  \n",
        "start_time = time.time()\n",
        "losses, errors_all, model = train_dist(2)\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "model_name = 'model_4_NoOptimizer_1000epochs_distOnly.h5'\n",
        "model.save(model_name)\n",
        "file = open('errors_all_0709_NoOptimizer_5000epochs.txt', 'w')\n",
        "for ers in errors_all:\n",
        "  file.write(\"\\n\")\n",
        "  file.write(str(ers))\n",
        "file.close()\n",
        "print(losses)\n",
        "print(errors_all)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x[:2]: [0.09762701 0.43037873]\n",
            "<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7f57202aea58>\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_4 (Dense)              (None, 5)                 10        \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 6         \n",
            "=================================================================\n",
            "Total params: 16\n",
            "Trainable params: 16\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTG_Q0ebRrm8",
        "colab_type": "code",
        "outputId": "cfc7f071-d33a-4b7e-b4b1-6b24d3c98480",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "print(\"Before training\")\n",
        "plot_errors(errors_all[0])\n",
        "print(\"After training\")\n",
        "plot_errors(errors_all[-1])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "After training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-4505304b7565>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"After training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplot_errors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrors_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'plot_errors' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwUDLI2qRvy9",
        "colab_type": "code",
        "outputId": "0855348a-987b-4576-d53c-1f018a7f304f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        ""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "After training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-7b32ccd78174>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"After training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplot_errors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrors_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'plot_errors' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11hQ5snUme8F",
        "colab_type": "code",
        "outputId": "6b246f8e-9d23-4649-c721-3ac10a58eb0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3s4r5ARn_Uk",
        "colab_type": "code",
        "outputId": "4719398e-78a5-4baf-9ddd-ce49c0ece180",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "%cd /content/gdrive/My\\ Drive/Colab\\ Notebooks/\n",
        "!pwd\n",
        "file = open('errors_all_0709_4_NoOptimizer.txt', 'w')\n",
        "for ers in errors_all:\n",
        "  file.write(\"\\n\")\n",
        "  file.write(str(ers))\n",
        "file.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Colab Notebooks\n",
            "/content/gdrive/My Drive/Colab Notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OW2gsK7qUwH-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q h5py pyyaml\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import os\n",
        "tf.__version__"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8p3_hDhNVO6z",
        "colab_type": "code",
        "outputId": "174568b4-5c75-4684-8061-bb034a6bf4bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "new_model = tf.keras.models.load_model('model_4_NoOptimizer_2000epochs_distOnly.h5')\n",
        "new_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_28 (Dense)             (None, 5)                 10        \n",
            "_________________________________________________________________\n",
            "dense_29 (Dense)             (None, 1)                 6         \n",
            "=================================================================\n",
            "Total params: 16\n",
            "Trainable params: 16\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_vfxp1vVzzJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4VJRT7uLbqH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}