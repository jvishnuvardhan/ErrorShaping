{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ceessv_clean.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"yAt18MTQ-Yso","colab_type":"code","colab":{}},"source":["from __future__ import absolute_import, division, print_function, unicode_literals\n","import os, datetime\n","import matplotlib.pyplot as plt\n","import numpy as np\n","!pip install -q tensorflow==2.0.0-beta1\n","import tensorflow as tf\n","%load_ext tensorboard"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ib7HOb6vU2WF","colab_type":"code","outputId":"95944391-489d-4d9f-84c4-c9c7dd431c71","executionInfo":{"status":"ok","timestamp":1562177603707,"user_tz":240,"elapsed":368,"user":{"displayName":"Yue Ma","photoUrl":"https://lh4.googleusercontent.com/--iSBd3eJMO0/AAAAAAAAAAI/AAAAAAAAAsg/0Oyuz9iN1ZQ/s64/photo.jpg","userId":"17313683663076417156"}},"colab":{"base_uri":"https://localhost:8080/","height":144}},"source":["train_dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/data/iris_training.csv\"\n","train_dataset_fp = tf.keras.utils.get_file(fname=os.path.basename(train_dataset_url),\n","                                           origin=train_dataset_url)\n","\n","column_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\n","\n","feature_names = column_names[:-1]\n","label_name = column_names[-1]\n","\n","\n","class_names = ['Iris setosa', 'Iris versicolor', 'Iris virginica']\n","batch_size = 32\n","\n","train_dataset = tf.data.experimental.make_csv_dataset(\n","    train_dataset_fp,\n","    batch_size,\n","    column_names=column_names,\n","    label_name=label_name,\n","    num_epochs=1)\n","\n","features, labels = next(iter(train_dataset))\n","\n","def pack_features_vector(features, labels):\n","  \"\"\"Pack the features into a single array.\"\"\"\n","  features = tf.stack(list(features.values()), axis=1)\n","  return features, labels\n","train_dataset = train_dataset.map(pack_features_vector)\n","features, labels = next(iter(train_dataset))\n","#print(features[:5])\n","model = tf.keras.Sequential([\n","  tf.keras.layers.Dense(10, activation=tf.nn.relu, input_shape=(4,)),  # input shape required\n","  tf.keras.layers.Dense(10, activation=tf.nn.relu),\n","  tf.keras.layers.Dense(3)\n","])\n","predictions = model(features)\n","#predictions[:5]\n","tf.nn.softmax(predictions[:5])\n","print(\"Prediction: {}\".format(tf.argmax(predictions, axis=1)))\n","print(\"    Labels: {}\".format(labels))\n","\n","def plot_bar(errors, num):\n","    plt.hist(errors, bins = 20)\n","    plt.xlabel(\"Prediction Error [MPG]\")\n","    title = \"epoch:\" + str(num)\n","    plt.title(title)\n","    _ = plt.ylabel(\"Count\")\n","    plt.show()\n","    plt.savefig(title)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["WARNING: Logging before flag parsing goes to stderr.\n","W0703 18:13:23.455134 140460800649088 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/experimental/ops/readers.py:498: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n"],"name":"stderr"},{"output_type":"stream","text":["Prediction: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n","    Labels: [0 2 2 2 1 0 0 2 1 0 1 0 1 0 0 0 1 1 0 1 2 1 0 2 0 0 2 1 0 2 0 0]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"aMSxXWTtVBm1","colab_type":"text"},"source":["## Performence without our loss function"]},{"cell_type":"code","metadata":{"id":"qecBdukfS5c0","colab_type":"code","colab":{}},"source":["from tensorflow.python.util.tf_export import keras_export\n","from tensorflow.python.keras.utils import losses_utils\n","from tensorflow.python.keras.utils import tf_utils\n","from tensorflow.python.keras import backend as K\n","import abc\n","\n","@keras_export('keras.losses.Loss')\n","class Loss(object):\n","\n","  def __init__(self, reduction=losses_utils.ReductionV2.AUTO, name=None):\n","    losses_utils.ReductionV2.validate(reduction)\n","    self.reduction = reduction\n","    self.name = name\n","\n","  def __call__(self, y_true, y_pred, sample_weight=None):\n","\n","    scope_name = 'lambda' if self.name == '<lambda>' else self.name\n","    graph_ctx = tf_utils.graph_context_for_symbolic_tensors(\n","        y_true, y_pred, sample_weight)\n","    with K.name_scope(scope_name or self.__class__.__name__), graph_ctx:\n","      losses = self.call(y_true, y_pred)\n","      return losses_utils.compute_weighted_loss(\n","          losses, sample_weight, reduction=self._get_reduction())\n","    \n","  @classmethod\n","  def from_config(cls, config):\n","    return cls(**config)\n","\n","  def get_config(self):\n","    return {'reduction': self.reduction, 'name': self.name}\n","\n","  @abc.abstractmethod\n","  @doc_controls.for_subclass_implementers\n","  def call(self, y_true, y_pred):\n","    NotImplementedError('Must be implemented in subclasses.')\n","\n","  def _get_reduction(self):\n","    if self.reduction == losses_utils.ReductionV2.AUTO:\n","      return losses_utils.ReductionV2.SUM_OVER_BATCH_SIZE\n","    return self.reduction\n","\n","class LossFunctionWrapper(Loss):\n","\n","  def __init__(self,\n","               fn,\n","               reduction=losses_utils.ReductionV2.NONE,\n","               name=None,\n","               **kwargs):\n","    super(LossFunctionWrapper, self).__init__(reduction=reduction, name=name)\n","    self.fn = fn\n","    self._fn_kwargs = kwargs\n","\n","  def call(self, y_true, y_pred):\n","    return self.fn(y_true, y_pred, **self._fn_kwargs)\n","\n","  def get_config(self):\n","    config = {}\n","    for k, v in six.iteritems(self._fn_kwargs):\n","      config[k] = K.eval(v) if tf_utils.is_tensor_or_variable(v) else v\n","    base_config = super(LossFunctionWrapper, self).get_config()\n","    return dict(list(base_config.items()) + list(config.items()))\n","\n","\n","class SparseCategoricalCrossentropy(LossFunctionWrapper):\n","  def __init__(self,\n","               from_logits=False,\n","               reduction=losses_utils.ReductionV2.NONE,\n","               name=None):\n","    super(SparseCategoricalCrossentropy, self).__init__(\n","        sparse_categorical_crossentropy,\n","        name=name,\n","        reduction=reduction,\n","        from_logits=from_logits)\n","    \n","def sparse_categorical_crossentropy(y_true, y_pred, from_logits=False, axis=-1):\n","  return K.sparse_categorical_crossentropy(\n","      y_true, y_pred, from_logits=from_logits, axis=axis)\n","    \n","x = SparseCategoricalCrossentropy()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"214CmbT1VJCS","colab_type":"code","outputId":"204c6124-12e3-4b1a-c2ea-b8fd1e7b11de","executionInfo":{"status":"ok","timestamp":1562166992353,"user_tz":240,"elapsed":28879,"user":{"displayName":"Yue Ma","photoUrl":"https://lh4.googleusercontent.com/--iSBd3eJMO0/AAAAAAAAAAI/AAAAAAAAAsg/0Oyuz9iN1ZQ/s64/photo.jpg","userId":"17313683663076417156"}},"colab":{"base_uri":"https://localhost:8080/","height":90}},"source":["def loss(model, x, y):\n","  y_ = model(x)\n","  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","  batch_loss = loss_object(y_true=y, y_pred=y_)\n","  loss_object = SparseCategoricalCrossentropy(from_logits=True)\n","  losses = loss_object(y_true=y, y_pred=y_)\n","  single_losses_list = [loss.numpy() for loss in losses]\n","  \n","  return tf.convert_to_tensor(batch_loss, dtype=np.float32), single_losses_list\n","\n","def grad(model, inputs, targets):\n","  with tf.GradientTape() as tape:\n","    batch_loss, single_losses_list = loss(model, inputs, targets)\n","  grads = tape.gradient(batch_loss, model.trainable_variables)\n","  return grads, batch_loss, single_losses_list\n","\n","\n","log_dir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n","print(\"logs file name:\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n","summary_writer = tf.summary.create_file_writer(logdir=log_dir)\n","optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n","\n","num_epochs = 500\n","train_loss_results = []\n","train_accuracy_results = []\n","\n","losses_ce_sample = []\n","losses_ce_batch = []\n","\n","for epoch in range(num_epochs):\n","  epoch_loss_avg = tf.keras.metrics.Mean()\n","  epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n"," \n","  for x, y in train_dataset:\n","    grads, loss_value, single_losses_list = grad(model, x, y)\n","    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n","    epoch_loss_avg(loss_value)\n","    epoch_accuracy(y, model(x))\n","    losses_ce_sample.append(single_losses_list)\n","    losses_ce_batch.append(loss_value)\n","\n","  train_loss_results.append(epoch_loss_avg.result()) \n","  train_accuracy_results.append(epoch_accuracy.result())\n","\n","  with summary_writer.as_default():\n","    tf.summary.scalar('epoch_loss_avg', epoch_loss_avg.result(), step=optimizer.iterations)\n","    tf.summary.scalar('epoch_accuracy', epoch_accuracy.result(), step=optimizer.iterations)\n","    \n","print(\"Epoch {:03d}: Loss: {:.3f}, Accuracy: {:.3%}\".format(epoch,epoch_loss_avg.result(), epoch_accuracy.result()))\n","losses_sample = [x for sublist in losses_ce_sample for x in sublist]\n","losses_sample = [format(x, 'f') for x in losses_sample]\n","print(\"len(losses_sample):\", len(losses_sample))\n","with open('losses_ce_sample.txt', 'w') as f:\n","    for item in losses_sample:\n","        f.write(\"%s\\n\" % item)\n","        \n","        \n","losses_ce_batch = [format(x.numpy(), 'f') for x in losses_ce_batch]\n","print(\"len(losses_ce_batch):\", len(losses_batch))\n","with open('losses_ce_batch.txt', 'w') as f:\n","    for item in losses_ce_batch:\n","        f.write(\"%s\\n\" % item)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["file name: 20190703-151603\n","Epoch 499: Loss: 0.032, Accuracy: 99.167%\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["'\\nlosses_sample = trunc_list(ce_single_losses_all)\\nprint(\"len(losses_sample):\", len(losses_sample))\\nwith open(\\'losses_ce_sample.txt\\', \\'w\\') as f:\\n    for item in losses_sample:\\n        f.write(\"%s\\n\" % item)\\n        \\n        \\nlosses_batch = [x.numpy() for x in losses_batch]\\nprint(\"len(losses_batch):\", len(losses_batch))\\nwith open(\\'losses_ce_batch.txt\\', \\'w\\') as f:\\n    for item in losses_batch:\\n        f.write(\"%s\\n\" % item)\\n        \\n'"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"7E4LXjLJ8mYW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"4d2018c2-61b1-4670-fa25-d75358e809e9","executionInfo":{"status":"ok","timestamp":1562176040457,"user_tz":240,"elapsed":342,"user":{"displayName":"Yue Ma","photoUrl":"https://lh4.googleusercontent.com/--iSBd3eJMO0/AAAAAAAAAAI/AAAAAAAAAsg/0Oyuz9iN1ZQ/s64/photo.jpg","userId":"17313683663076417156"}}},"source":["\n","print(\"len(losses_cees_sample):\", len(losses_cees_sample))\n","with open('losses_cees_sample.txt', 'w') as f:\n","    for item in losses_cees_sample:\n","        f.write(\"%s\\n\" % item)\n","        \n","\n","print(\"len(losses_cees_batch):\", len(losses_cees_batch))\n","with open('losses_cees_batch.txt', 'w') as f:\n","    for item in losses_cees_batch:\n","        f.write(\"%s\\n\" % item)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["len(losses_cees_sample): 480000\n","len(losses_cees_batch): 2000\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SJLwTp0hVmhF","colab_type":"text"},"source":["## Performence with our loss function"]},{"cell_type":"code","metadata":{"id":"eqJt2qBAVnNT","colab_type":"code","outputId":"df96c11b-0952-4e2f-daa1-6cd6a3a59c34","executionInfo":{"status":"ok","timestamp":1562178480618,"user_tz":240,"elapsed":138956,"user":{"displayName":"Yue Ma","photoUrl":"https://lh4.googleusercontent.com/--iSBd3eJMO0/AAAAAAAAAAI/AAAAAAAAAsg/0Oyuz9iN1ZQ/s64/photo.jpg","userId":"17313683663076417156"}},"colab":{"base_uri":"https://localhost:8080/","height":88}},"source":["def MMD1d(x1, x2):\n","  dis = sum([x**2 for x in x1])/len(x1) - sum([x**2 for x in x2])/len(x2)\n","  return dis**2\n","\n","def get_MMD_norm(errors, sigma=0.1): \n","  x2 = np.random.normal(0, sigma, 100)\n","  loss = MMD1d(errors, x2)\n","  return loss\n","\n","def loss(model, x, y, sigma=0.1):\n","  y_ = model(x) \n","  losses = []\n","  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","  for i in range(y.shape[0]):\n","    loss = loss_object(y_true=y[i], y_pred=y_[i])\n","    losses.append(loss) \n","  batch_loss = get_MMD_norm(losses)\n","  single_losses_list = [loss.numpy() for loss in losses]\n","  return tf.convert_to_tensor(batch_loss, dtype=np.float32), single_losses_list\n","\n","def grad(model, inputs, targets, sigma=0.1):\n","  with tf.GradientTape() as tape:\n","    tape.watch(model.trainable_variables)\n","    batch_loss, single_losses = loss(model, inputs, targets, sigma=0.1)\n","  return tape.gradient(batch_loss, model.trainable_variables), batch_loss, single_losses \n","\n","\n","log_dir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n","print(\"logs file name:\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n","summary_writer = tf.summary.create_file_writer(logdir=log_dir)\n","optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n","\n","num_epochs = 500\n","train_loss_results = []\n","train_accuracy_results = []\n","\n","losses_cees_sample = []\n","losses_cees_batch = []\n","for epoch in range(num_epochs):\n","  epoch_loss_avg = tf.keras.metrics.Mean()\n","  epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n","\n","  for x, y in train_dataset:\n","    # Optimize the model\n","    grads, batch_loss, single_losses = grad(model, x, y)\n","    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n","    epoch_loss_avg(batch_loss)\n","    epoch_accuracy(y, model(x))\n","    losses_cees_sample.append(single_losses)\n","    losses_cees_batch.append(batch_loss)\n","    \n","  tf.summary.trace_on(graph=True, profiler=True)  \n","  with summary_writer.as_default():\n","    tf.summary.scalar('epoch_loss_avg', epoch_loss_avg.result(), step=optimizer.iterations)\n","    tf.summary.scalar('epoch_accuracy', epoch_accuracy.result(), step=optimizer.iterations)\n","    \n","    tf.summary.trace_export(name=\"my_func_trace\",step=0,profiler_outdir=log_dir)\n","\n","  # end epoch\n","  train_loss_results.append(epoch_loss_avg.result())\n","  train_accuracy_results.append(epoch_accuracy.result())\n","  \n","\n","print(\"Epoch {:03d}: Loss: {:.3f}, Accuracy: {:.3%}\".format(epoch,epoch_loss_avg.result(), epoch_accuracy.result()))\n","losses_cees_sample = [x for sublist in losses_cees_sample for x in sublist]\n","losses_cees_sample = [format(x, 'f') for x in losses_cees_sample]\n","print(\"len(losses_cees_sample):\", len(losses_cees_sample))\n","with open('losses_cees_sample.txt', 'w') as f:\n","    for item in losses_cees_sample:\n","      f.write(\"%s\\n\" % item)\n","        \n","        \n","losses_cees_batch = [format(x.numpy(), 'f') for x in losses_cees_batch]\n","print(\"len(losses_cees_batch):\", len(losses_cees_batch))\n","with open('losses_cees_batch.txt', 'w') as f:\n","    for item in losses_cees_batch:\n","        f.write(\"%s\\n\" % item)"],"execution_count":29,"outputs":[{"output_type":"stream","text":["logs file name: 20190703-182541\n","Epoch 499: Loss: 0.002, Accuracy: 98.333%\n","len(losses_cees_sample): 60000\n","len(losses_cees_batch): 2000\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vNQqdnYydBMT","colab_type":"text"},"source":["## The performence with our loss function and shrinking variance"]},{"cell_type":"code","metadata":{"id":"1BsfSRXHdBdz","colab_type":"code","outputId":"5ebfc96d-d6e3-4e72-ec9e-843dac5c9ee7","executionInfo":{"status":"ok","timestamp":1562178890236,"user_tz":240,"elapsed":138898,"user":{"displayName":"Yue Ma","photoUrl":"https://lh4.googleusercontent.com/--iSBd3eJMO0/AAAAAAAAAAI/AAAAAAAAAsg/0Oyuz9iN1ZQ/s64/photo.jpg","userId":"17313683663076417156"}},"colab":{"base_uri":"https://localhost:8080/","height":88}},"source":["log_dir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n","print(datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n","summary_writer = tf.summary.create_file_writer(logdir=log_dir)\n","optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n","\n","patience_n = 25\n","patience = patience_n\n","sigma=0.1\n","num_epochs = 500\n","\n","train_loss_results = []\n","train_accuracy_results = []\n","\n","losses_ceessv_sample = []\n","losses_ceessv_batch = []\n","for epoch in range(num_epochs):\n","  epoch_loss_avg = tf.keras.metrics.Mean()\n","  epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n","  if patience <= 0: \n","    sigma/=2\n","    patience = patience_n\n","  # Training loop - using batches of 32\n","  for x, y in train_dataset:\n","    grads, batch_loss, single_losses = grad(model, x, y, sigma)\n","    losses_ceessv_sample.append(single_losses)\n","    losses_ceessv_batch.append(batch_loss)\n","    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n","\n","    # Track progress\n","    epoch_loss_avg(batch_loss)  # add current batch loss\n","    # compare predicted label to actual label\n","    epoch_accuracy(y, model(x))\n","    \n","  tf.summary.trace_on(graph=True, profiler=True)\n","  # end epoch\n","  train_loss_results.append(epoch_loss_avg.result())\n","  train_accuracy_results.append(epoch_accuracy.result())\n","  \n","  with summary_writer.as_default():\n","    tf.summary.scalar('epoch_loss_avg', epoch_loss_avg.result(), step=optimizer.iterations)\n","    tf.summary.scalar('epoch_accuracy', epoch_accuracy.result(), step=optimizer.iterations)\n","    \n","    tf.summary.trace_export(name=\"my_func_trace\",step=0,profiler_outdir=log_dir)\n","      \n","      \n","  \n","  if epoch>1 and train_loss_results[-1].numpy() > train_loss_results[-2].numpy():\n","    patience -=1\n","    \n","    \n","print(\"Epoch {:03d}: Loss: {:.3f}, Accuracy: {:.3%}\".format(epoch,epoch_loss_avg.result(), epoch_accuracy.result()))\n","losses_ceessv_sample = [x for sublist in losses_ceessv_sample for x in sublist]\n","losses_ceessv_sample = [format(x, 'f') for x in losses_ceessv_sample]\n","print(\"len(losses_ceessv_sample):\", len(losses_ceessv_sample))\n","with open('losses_ceessv_sample.txt', 'w') as f:\n","    for item in losses_ceessv_sample:\n","        f.write(\"%s\\n\" % item)\n","        \n","        \n","losses_ceessv_batch = [format(x.numpy(), 'f') for x in losses_ceessv_batch]\n","print(\"len(losses_ceessv_batch):\", len(losses_ceessv_batch))\n","with open('losses_ceessv_batch.txt', 'w') as f:\n","    for item in losses_ceessv_batch:\n","        f.write(\"%s\\n\" % item)"],"execution_count":31,"outputs":[{"output_type":"stream","text":["20190703-183231\n","Epoch 499: Loss: 0.000, Accuracy: 99.167%\n","len(losses_ceessv_sample): 60000\n","len(losses_ceessv_batch): 2000\n"],"name":"stdout"}]}]}