MSE actually is to minimize the variance of errors, which is only part of the information of a distribution. Gaussian has the highest entropy and is typically assumed to reflect the aleatoric uncertainty of the prediction. This prediction is also called irreducible uncertainty because it is the intrinsic noise of data, can not be improved by collecting more data. Nudging the error distribution to be Normal should minimize the epistemic uncertainty caused by model structure and parameters. the logic is that we have the prior knowledge what irreducible errors should look like, so we inject this knowledge into our network so we can push the model to focus on the part we can improve and leave the part we know we cannot improve.
